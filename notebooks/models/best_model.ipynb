{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc0bc8d0",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "110b1e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_rel\n",
    "from itertools import combinations\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1757f6b2",
   "metadata": {},
   "source": [
    "# Load saved metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0095d816",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../metrics/model_performance_across_20k_datasets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a268917",
   "metadata": {},
   "source": [
    "# Generate summary statistics for all models\n",
    "\n",
    "For each metric (weighted_f1, accuracy1, accuracy2), find mean, std, cv for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bd08f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "weighted_f1_score_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "weighted_f1_score_std",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "accuracy_1_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "accuracy_1_std",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "accuracy_2_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "accuracy_2_std",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "weighted_f1_score_cv",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "accuracy_1_cv",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "accuracy_2_cv",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "018fcbe7-5a23-425e-a5b4-20e8e21b7d13",
       "rows": [
        [
         "random_forest",
         "0.6214180478989475",
         "0.01238054419252821",
         "0.52855",
         "0.014427586879235753",
         "0.7532",
         "0.016366203945293725",
         "0.019923052177817476",
         "0.027296541252929248",
         "0.02172889530708142"
        ],
        [
         "xgboost",
         "0.6723708277282253",
         "0.012739491952352302",
         "0.5864499999999999",
         "0.015702748669881767",
         "0.782",
         "0.01762474337252514",
         "0.018947121777123932",
         "0.026775937709748093",
         "0.022538035003229078"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weighted_f1_score_mean</th>\n",
       "      <th>weighted_f1_score_std</th>\n",
       "      <th>accuracy_1_mean</th>\n",
       "      <th>accuracy_1_std</th>\n",
       "      <th>accuracy_2_mean</th>\n",
       "      <th>accuracy_2_std</th>\n",
       "      <th>weighted_f1_score_cv</th>\n",
       "      <th>accuracy_1_cv</th>\n",
       "      <th>accuracy_2_cv</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>random_forest</th>\n",
       "      <td>0.621418</td>\n",
       "      <td>0.012381</td>\n",
       "      <td>0.52855</td>\n",
       "      <td>0.014428</td>\n",
       "      <td>0.7532</td>\n",
       "      <td>0.016366</td>\n",
       "      <td>0.019923</td>\n",
       "      <td>0.027297</td>\n",
       "      <td>0.021729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgboost</th>\n",
       "      <td>0.672371</td>\n",
       "      <td>0.012739</td>\n",
       "      <td>0.58645</td>\n",
       "      <td>0.015703</td>\n",
       "      <td>0.7820</td>\n",
       "      <td>0.017625</td>\n",
       "      <td>0.018947</td>\n",
       "      <td>0.026776</td>\n",
       "      <td>0.022538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               weighted_f1_score_mean  weighted_f1_score_std  accuracy_1_mean  \\\n",
       "model                                                                           \n",
       "random_forest                0.621418               0.012381          0.52855   \n",
       "xgboost                      0.672371               0.012739          0.58645   \n",
       "\n",
       "               accuracy_1_std  accuracy_2_mean  accuracy_2_std  \\\n",
       "model                                                            \n",
       "random_forest        0.014428           0.7532        0.016366   \n",
       "xgboost              0.015703           0.7820        0.017625   \n",
       "\n",
       "               weighted_f1_score_cv  accuracy_1_cv  accuracy_2_cv  \n",
       "model                                                              \n",
       "random_forest              0.019923       0.027297       0.021729  \n",
       "xgboost                    0.018947       0.026776       0.022538  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# List of metrics to analyze\n",
    "metrics = ['weighted_f1_score', 'accuracy_1', 'accuracy_2']\n",
    "\n",
    "# Summary statistics per model\n",
    "summary = df.groupby('model')[metrics].agg(['mean', 'std'])\n",
    "summary['weighted_f1_score', 'cv'] = summary['weighted_f1_score', 'std'] / summary['weighted_f1_score', 'mean']\n",
    "summary['accuracy_1', 'cv'] = summary['accuracy_1', 'std'] / summary['accuracy_1', 'mean']\n",
    "summary['accuracy_2', 'cv'] = summary['accuracy_2', 'std'] / summary['accuracy_2', 'mean']\n",
    "\n",
    "# Flatten column names\n",
    "summary.columns = ['_'.join(col).strip() for col in summary.columns.values]\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27434655",
   "metadata": {},
   "source": [
    "# Paired t-test for F1 score\n",
    "\n",
    "The paired t-test tests whether the mean difference between the two models' F1 scores is significantly different from zero. Here's what we do:\n",
    "1. Perform the t-test:\n",
    "- Compare the models' weighted F1 scores on the same datasets.\n",
    "2. Interpret the p-value:\n",
    "- If p-value < 0.05, there is a statistically significant difference in F1 scores between the two models.\n",
    "- If p-value â‰¥ 0.05, there is no significant difference, and the models are performing similarly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad38b43e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "model_a",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "model_b",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "mean_a",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean_b",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "better_model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "t_stat",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "p_value",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "significant",
         "rawType": "bool",
         "type": "boolean"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "6e1ea976-264f-468f-bf65-b1090670e5f9",
       "rows": [
        [
         "0",
         "xgboost",
         "random_forest",
         "0.6723708277282252",
         "0.6214180478989475",
         "xgboost",
         "21.667593097956534",
         "7.391096112542572e-15",
         "True"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>mean_a</th>\n",
       "      <th>mean_b</th>\n",
       "      <th>better_model</th>\n",
       "      <th>t_stat</th>\n",
       "      <th>p_value</th>\n",
       "      <th>significant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.672371</td>\n",
       "      <td>0.621418</td>\n",
       "      <td>xgboost</td>\n",
       "      <td>21.667593</td>\n",
       "      <td>7.391096e-15</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_a        model_b    mean_a    mean_b better_model     t_stat  \\\n",
       "0  xgboost  random_forest  0.672371  0.621418      xgboost  21.667593   \n",
       "\n",
       "        p_value  significant  \n",
       "0  7.391096e-15         True  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Assuming df looks like: model | dataset | weighted_f1_score\n",
    "\n",
    "results = []\n",
    "models = df['model'].unique()\n",
    "\n",
    "# Step 1: Run pairwise t-tests\n",
    "for model_a, model_b in combinations(models, 2):\n",
    "    scores_a = df[df['model'] == model_a][\"weighted_f1_score\"].values\n",
    "    scores_b = df[df['model'] == model_b][\"weighted_f1_score\"].values\n",
    "\n",
    "    stat, p = ttest_rel(scores_a, scores_b)\n",
    "\n",
    "    mean_a = np.mean(scores_a)\n",
    "    mean_b = np.mean(scores_b)\n",
    "\n",
    "    # Who's better\n",
    "    better = model_a if mean_a > mean_b else model_b\n",
    "\n",
    "    results.append({\n",
    "        'model_a': model_a,\n",
    "        'model_b': model_b,\n",
    "        'mean_a': mean_a,\n",
    "        'mean_b': mean_b,\n",
    "        'better_model': better,\n",
    "        't_stat': stat,\n",
    "        'p_value': p,\n",
    "        'significant': p < 0.05\n",
    "    })\n",
    "\n",
    "df_compare = pd.DataFrame(results)\n",
    "df_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee4ec9b",
   "metadata": {},
   "source": [
    "# Pick the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0287568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Ranking by Significant Wins:\n",
      "better_model\n",
      "xgboost    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Best overall model: xgboost\n",
      "\n",
      " Mean F1 Score per Model:\n",
      "model\n",
      "xgboost          0.672371\n",
      "random_forest    0.621418\n",
      "Name: weighted_f1_score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Filter only significant comparisons\n",
    "significant = df_compare[df_compare['significant']]\n",
    "\n",
    "# Count how many times each model was significantly better\n",
    "win_counts = significant['better_model'].value_counts()\n",
    "\n",
    "# Show ranking\n",
    "print(\"Model Ranking by Significant Wins:\")\n",
    "print(win_counts)\n",
    "\n",
    "# Pick the top model\n",
    "best_model = win_counts.idxmax()\n",
    "print(f\"\\nBest overall model: {best_model}\")\n",
    "\n",
    "# Mean F1 score per model across all datasets\n",
    "mean_f1 = df.groupby(\"model\")[\"weighted_f1_score\"].mean().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\n Mean F1 Score per Model:\")\n",
    "print(mean_f1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
